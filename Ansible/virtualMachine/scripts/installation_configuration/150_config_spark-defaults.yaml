---
- hosts: master
  tasks:
    - name: Create the Spark configuration file ('spark-defaults.conf')
      copy:
        src: /opt/hadoop/spark/conf/spark-defaults.conf.template
        dest: /opt/hadoop/spark/conf/spark-defaults.conf
        remote_src: yes
      become: true
      become_user: root


    - name: Update the Spark configuration file with the necessary configuration
      blockinfile:
        path: /opt/hadoop/spark/conf/spark-defaults.conf
        block: |
          spark.master                      yarn
          spark.driver.memory               512m
          spark.yarn.am.memory              512m
          spark.executor.memory             512m
          spark.eventLog.enabled            true
          spark.eventLog.dir                hdfs://master:9000/spark-logs
          spark.history.provider            org.apache.spark.deploy.history.FsHistoryProvider
          spark.history.fs.logDirectory     hdfs://master:9000/spark-logs
          spark.history.fs.update.interval  10s
          spark.history.ui.port             18080          
          spark.executors.cores             5
          spark.executor.extraJavaOptions   -XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'
          spark.driver.extraJavaOptions     -XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'
      become: true
      become_user: root